{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d xhlulu/140k-real-and-fake-faces\n",
    "!unzip 140k-real-and-fake-faces.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "from keras.optimizers import legacy\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = './real_vs_fake/real-vs-fake/'\n",
    "\n",
    "image_gen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_flow = image_gen.flow_from_directory(\n",
    "    datapath + 'train/', target_size=(224, 224), batch_size=batch_size, class_mode='binary'\n",
    ")\n",
    "valid_flow = image_gen.flow_from_directory(\n",
    "    datapath + 'valid/', target_size=(224, 224), batch_size=batch_size, class_mode='binary'\n",
    ")\n",
    "test_flow = image_gen.flow_from_directory(\n",
    "    datapath + 'test/', target_size=(224, 224), batch_size=1, shuffle=False, class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(path, set_):\n",
    "    dir_ = os.path.join(path, 'train', set_)\n",
    "    k = np.random.randint(20000)\n",
    "    fig, ax = plt.subplots(3,3, figsize=(8,8))\n",
    "    for j in range(3):\n",
    "        for i in range(3):\n",
    "            img = load_img(os.path.join(dir_, os.listdir(os.path.join(dir_))[k]))\n",
    "            ax[j,i].imshow(img)\n",
    "            ax[j,i].set_title(\"\")\n",
    "            ax[j,i].axis('off')\n",
    "            k = np.random.randint(20000)\n",
    "    plt.suptitle(set_ + ' faces from train dataset')\n",
    "    return plt\n",
    "\n",
    "def plot_loss(epochs, loss, val_loss):\n",
    "    plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'orange', label = 'Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(epochs, acc, val_acc):\n",
    "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'orange', label = 'Validation accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def predict_img_class():\n",
    "    k = np.random.randint(20000)\n",
    "    gridx = 4\n",
    "    gridy = 4\n",
    "    fig, ax = plt.subplots(gridx,gridy, figsize=(14,14))\n",
    "    for j in range(gridx):\n",
    "        for i in range(gridy):\n",
    "            if test_flow.classes[k] == 0:\n",
    "                actual = \"FAKE\"\n",
    "            else:\n",
    "                actual = \"REAL\"\n",
    "            if model.predict(test_flow[k][0])[0][0]<0.5:\n",
    "                prediction = \"FAKE\"\n",
    "            else:\n",
    "                prediction = \"REAL\"\n",
    "            datasetpath = test_flow.filenames[k]\n",
    "            imgpath = datapath+'test/'+datasetpath\n",
    "            img = load_img(imgpath)\n",
    "            ax[j,i].imshow(img)\n",
    "            ax[j,i].set_title(f\"Predicted: {prediction}, Actual: {actual}\",size = 12)\n",
    "            ax[j,i].axis('off')\n",
    "            k = np.random.randint(20000)\n",
    "    fig.suptitle('Testing the prediction on Test images', size = 20)\n",
    "    return plt\n",
    "\n",
    "    from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Conv2DTranspose, BatchNormalization, LeakyReLU\n",
    "\n",
    "def build_generator(latent_dim=100):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    \n",
    "    model.add(Dense(7 * 7 * 256, input_dim=latent_dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 256)))\n",
    "\n",
    "    \n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2DTranspose(32, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2DTranspose(16, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    \n",
    "    model.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='tanh'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(datapath, 'real').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(datapath, 'fake').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224,224,3)\n",
    "epsilon=0.001\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=input_shape))\n",
    "model.add(Conv2D(filters=16, kernel_size=3, activation= 'relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(filters=1024, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = legacy.Adam(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='binary_crossentropy',optimizer= opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"saved-final-model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 80000//64\n",
    "valid_steps = 5000//64\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_flow,\n",
    "    epochs =4,\n",
    "    callbacks= callbacks_list,\n",
    "    steps_per_epoch = train_steps,\n",
    "    validation_data = valid_flow,\n",
    "    validation_steps = valid_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"saved-final-model-1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(range(1, len(loss) + 1), loss, val_loss)\n",
    "plot_accuracy(range(1, len(loss) + 1), acc, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_flow)\n",
    "y_test = test_flow.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = confusion_matrix(y_test, y_pred > 0.5)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = ['{0:0.0f}'.format(value) for value in results.flatten()]\n",
    "group_percentages = ['{0:.2%}'.format(value) for value in results.flatten()/np.sum(results)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(results, annot=labels, fmt='', cmap = 'Blues')\n",
    "plt.ylabel('Actual label', size = 20)\n",
    "plt.xlabel('Predicted label', size = 20)\n",
    "plt.xticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
    "plt.yticks(np.arange(2), ['Fake', 'Real'], size = 16)\n",
    "print('Accuracy Score :',accuracy_score(y_test, y_pred > 0.5))\n",
    "print('Report : ')\n",
    "print(classification_report(y_test, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC-AUC Score:\", metrics.roc_auc_score(y_test, y_pred))\n",
    "print(\"AP Score:\", metrics.average_precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.5f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_img_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = tf.keras.models.load_model('saved-final-model.h5')\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "generator = build_generator(latent_dim)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = Input(shape=(noise_dim,))\n",
    "generated_image = generator(noise)\n",
    "generated_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = tf.keras.models.load_model('saved-final-model-1.h5')\n",
    "\n",
    "\n",
    "latent_dim = 100\n",
    "generator = build_generator(latent_dim)\n",
    "\n",
    "\n",
    "discriminator.trainable = False\n",
    "gan_input = tf.keras.Input(shape=(latent_dim,))\n",
    "generated_image = generator(gan_input)\n",
    "gan_output = discriminator(generated_image)\n",
    "gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "\n",
    "\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_per_epoch = len(train_flow)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx in range(batch_per_epoch):\n",
    "\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "\n",
    "\n",
    "        generated_images = generator.predict(noise)\n",
    "\n",
    "\n",
    "        X = np.concatenate((train_flow.next()[0], generated_images))\n",
    "        y = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))))\n",
    "\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
    "        y = np.ones((batch_size, 1))\n",
    "\n",
    "        g_loss = gan.train_on_batch(noise, y)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Generator Loss: {g_loss}\")\n",
    "\n",
    "\n",
    "test_loss = gan.evaluate(test_flow)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
